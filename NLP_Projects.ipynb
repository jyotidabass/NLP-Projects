{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1Kj19mRNPvKuYLXgFwSTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/NLP-Projects/blob/main/NLP_Projects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 1: Text Classification**"
      ],
      "metadata": {
        "id": "wI1EpCWTz7tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b0E_Xyiz1V8",
        "outputId": "207e5080-5dc4-4397-8f9f-d59f29ed3896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load data\n",
        "train_data = [\"This is a positive review.\", \"This is a negative review.\"]\n",
        "train_labels = [1, 0]\n",
        "\n",
        "# Tokenize text\n",
        "nltk.download('punkt')\n",
        "tokenized_train_data = [word_tokenize(text) for text in train_data]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project 2: Sentiment Analysis**"
      ],
      "metadata": {
        "id": "rE8jJOwI0RPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Load data\n",
        "text = \"I love this product! It's amazing.\"\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Analyze sentiment\n",
        "sentiment = sia.polarity_scores(text)\n",
        "\n",
        "# Print sentiment scores\n",
        "print(\"Positive sentiment:\", sentiment['pos'])\n",
        "print(\"Negative sentiment:\", sentiment['neg'])\n",
        "print(\"Neutral sentiment:\", sentiment['neu'])\n",
        "print(\"Compound sentiment:\", sentiment['compound'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwRNa1N-0SuZ",
        "outputId": "4666c626-9ffc-4dd6-ee65-a191049a70b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sentiment: 0.734\n",
            "Negative sentiment: 0.0\n",
            "Neutral sentiment: 0.266\n",
            "Compound sentiment: 0.8516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 3: Named Entity Recognition**"
      ],
      "metadata": {
        "id": "QqQr_PNF0kAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load data\n",
        "text = \"Apple is a technology company based in California.\"\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Print entities\n",
        "print(\"Named entities:\", entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b6h5eO80lsj",
        "outputId": "08f145cc-2a71-4b93-ba9d-c6b2099d24af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities: [('Apple', 'ORG'), ('California', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project 4: Language Modeling**"
      ],
      "metadata": {
        "id": "ZoE82tIH0sqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load data\n",
        "text = \"This is a sample text.\"\n",
        "\n",
        "# Tokenize text\n",
        "tokens = text.split()\n",
        "\n",
        "# Create vocabulary mapping\n",
        "word_to_index = {token: index for index, token in enumerate(set(tokens))}\n",
        "# Map tokens to indices\n",
        "indexed_tokens = [word_to_index[token] for token in tokens]\n",
        "\n",
        "# Create language model\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = LanguageModel(len(word_to_index), 128, 128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train model\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    # Pass indexed tokens as input to the model\n",
        "    output = model(torch.tensor([indexed_tokens]))\n",
        "    # Use the index of the last token as the target\n",
        "    loss = criterion(output, torch.tensor([indexed_tokens[-1]]))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkyfq6Ks1CnF",
        "outputId": "8caac54e-110f-4ed4-ac95-d9c24fa6d5a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 1.6580555438995361\n",
            "Epoch: 1 Loss: 0.9252477288246155\n",
            "Epoch: 2 Loss: 0.4701130986213684\n",
            "Epoch: 3 Loss: 0.23719075322151184\n",
            "Epoch: 4 Loss: 0.12714453041553497\n",
            "Epoch: 5 Loss: 0.07412440329790115\n",
            "Epoch: 6 Loss: 0.0468980111181736\n",
            "Epoch: 7 Loss: 0.031855370849370956\n",
            "Epoch: 8 Loss: 0.02295481227338314\n",
            "Epoch: 9 Loss: 0.01736520044505596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 5: Text Generation**"
      ],
      "metadata": {
        "id": "wsafP9tO1NDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load data\n",
        "text = \"This is a sample text.\"\n",
        "\n",
        "# Tokenize text\n",
        "tokens = text.split()\n",
        "\n",
        "# Create a mapping from words to indices and vice-versa\n",
        "word_to_index = {token: index for index, token in enumerate(tokens)}\n",
        "index_to_word = {index: token for token, index in word_to_index.items()}\n",
        "\n",
        "# Create text generator\n",
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Initialize model and optimizer\n",
        "vocab_size = len(tokens)  # or len(word_to_index), they are the same\n",
        "model = TextGenerator(vocab_size, 128, 128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train model\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Convert tokens to numerical indices before feeding to the model\n",
        "    input_indices = torch.tensor([[word_to_index[token] for token in tokens]])\n",
        "\n",
        "    output = model(input_indices)\n",
        "\n",
        "    # Use the index of the last token as the target\n",
        "    target_index = torch.tensor([word_to_index[tokens[-1]]])\n",
        "\n",
        "    loss = criterion(output, target_index)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())\n",
        "\n",
        "# Generate text\n",
        "def generate_text(model, start_token, max_length):\n",
        "    # Get the index of the start token\n",
        "    start_token_index = word_to_index[start_token]\n",
        "\n",
        "    tokens_indices = [start_token_index]\n",
        "    for i in range(max_length):\n",
        "        output = model(torch.tensor([tokens_indices]))\n",
        "        token_index = torch.argmax(output).item()\n",
        "        tokens_indices.append(token_index)\n",
        "\n",
        "    # Convert indices back to words\n",
        "    generated_tokens = [index_to_word[index] for index in tokens_indices]\n",
        "\n",
        "    return generated_tokens\n",
        "\n",
        "print(\"Generated text:\", generate_text(model, tokens[0], 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqCzpA-I1cTM",
        "outputId": "fd98376d-c730-4342-ecf0-7505b8316f9d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 1.68317711353302\n",
            "Epoch: 1 Loss: 1.013279676437378\n",
            "Epoch: 2 Loss: 0.5608100295066833\n",
            "Epoch: 3 Loss: 0.30185121297836304\n",
            "Epoch: 4 Loss: 0.16795217990875244\n",
            "Epoch: 5 Loss: 0.09989055246114731\n",
            "Epoch: 6 Loss: 0.0639154389500618\n",
            "Epoch: 7 Loss: 0.04370548203587532\n",
            "Epoch: 8 Loss: 0.03161435201764107\n",
            "Epoch: 9 Loss: 0.023955313488841057\n",
            "Generated text: ['This', 'sample', 'text.', 'text.', 'text.', 'text.', 'text.', 'text.', 'text.', 'text.', 'text.']\n"
          ]
        }
      ]
    }
  ]
}